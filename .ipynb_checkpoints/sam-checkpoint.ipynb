{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping starts\n",
      "junhuihe login in successfully\n",
      "publication type is set\n",
      "source is set\n",
      "time range is set\n",
      "subject is set\n",
      "we'll scrape down 6082 files related to 'ticker(ORCL)'\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"d38ce785-2ae0-4db4-ad1b-f47043418c93\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page1_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"3e27b991-f9cf-4211-aa2d-48115ef9f7d1\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page2_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"b70a54d7-6e97-4e41-9dd0-96bf39470e84\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page3_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"5b110452-4131-42e1-9b78-4ab69a7b5676\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page4_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"274d3679-26d7-4ac7-90cc-08da6313efc1\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page5_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"dde58c50-999d-4d81-841a-22109752b2e4\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page6_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"8e183b69-5c76-4d03-845b-28f13bff64e7\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page7_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"d1e09e98-0aa3-4da7-aa7c-bac4c8d57885\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page8_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"1673e7f2-6855-4af3-bb16-f0f1547ba7df\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page9_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"24413a90-b2d2-4230-b2ad-683b2ee66f5b\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page10_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"93d2d4e3-ebaf-44ff-a96f-1c4f3f533fe3\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page11_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"eaea3d2c-fa09-46b1-946b-2c64da5cd234\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page12_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"8d910807-dac6-4397-8b16-e80b2268833f\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page13_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"9cb31516-829c-450f-979b-40a3b1bb3385\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page14_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"5b5c97af-471e-4888-9ab7-2f0c0dc5ecb8\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page15_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"55e73c3c-e42e-48d4-b956-3a5b93907665\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page16_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"f618c6a7-d319-4d10-8350-3d566b5165e9\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page17_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"dcd60da7-fa7b-4bce-8e4c-18f77ebc1f07\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page18_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"8f977ee1-cb72-4a15-8883-29418897a16e\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page19_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"36283124-0841-40a3-aef0-81bf3d33ee29\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page20_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"7cb56fb5-0008-4641-bdae-9cdfc5a0ec12\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page21_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"9bf046ed-8e4c-49fb-bac2-8d22deba527f\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page22_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"be59dd42-d099-47ea-a238-18b035972fe2\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page23_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"e09fc8be-ba4e-4f7c-af01-cacdce741a4b\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page24_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"6934145e-a5de-4450-b8fe-baf688b379ed\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page25_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"bce0ddff-9cf1-4f9b-8941-8277f3d5a165\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page26_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"36ef906d-e4b8-488e-a387-ebdc04bc7322\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page27_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"3073fdfa-9e3d-4a7e-b56c-9c8664193654\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page28_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"09d2c9ee-7c58-4e47-a18f-ead07f81ea3c\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page29_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"330656cb-d0bc-40e0-8b7c-692711b697c9\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page30_609\n",
      "here\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"8d8801faa9e7612afe7ff758d412ff0c\", element=\"52b5140e-25e8-4ae8-8f7a-20e793b919a1\")>\n",
      "6,082 results for ticker(ORCL) (narrowed)\n",
      "finishing page31_609\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 12 17:31:39 2018\n",
    "\n",
    "@author: chongshu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from xml.etree.cElementTree import XML\n",
    "import sys\n",
    "\n",
    "month_d = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "#searchTerms = r'JP Morgan'\n",
    "searchTermsList = [r'oracle', r'morgan stanley']\n",
    "searchTermsList = map(lambda x: f'ticker({x})' ,tickers[:10])\n",
    "url = r'https://libproxy.usc.edu/login?url=http://www.nexisuni.com'\n",
    "# url = r'http://libguides.usc.edu/go.php?c=9232127'\n",
    "username = 'junhuihe'\n",
    "password = 'ivy930322jasper951128'\n",
    "# username = 'MyUSCPassUsername'\n",
    "# password = 'MyUSCPassWord'\n",
    "#root = r'/Users/jasper/Documents/scrape_help/LexisNexis-Scraping'\n",
    "root = r'C:\\Users\\econguest\\Desktop\\scraper'\n",
    "path_to_chromedriver = root + r'\\chromedriver.exe'\n",
    "#download_folder = root + r'\\{}\\download'.format(searchTerms)\n",
    "# path_to_chromedriver = root + r'\\chromedriver'\n",
    "# download_folder = root + r'\\download'\n",
    "# dead_time = 300\n",
    "dead_time = 300\n",
    "\n",
    "\n",
    "def download_file(searchTerms, download_folder, url = url, username = username, \\\n",
    "                  dead_time = dead_time, path_to_chromedriver=path_to_chromedriver):\n",
    "    while True:\n",
    "        try:\n",
    "            # chromeOptions = webdriver.ChromeOptions()\n",
    "            prefs = {\"download.default_directory\" : download_folder + r'\\temp'}\n",
    "            # chromeOptions.add_argument('headless')\n",
    "            chromeOptions = webdriver.ChromeOptions()\n",
    "            # chromeOptions.add_argument(\"--window-size=1800,1000\")\n",
    "            # chromeOptions.add_argument(\"--disable-extensions\")\n",
    "            # chromeOptions.add_argument(\"--proxy-server='direct://'\")\n",
    "            # chromeOptions.add_argument(\"--proxy-bypass-list=*\")\n",
    "            # chromeOptions.add_argument(\"--start-maximized\")\n",
    "            # chromeOptions.add_argument('--headless')\n",
    "            # chromeOptions.add_argument('--disable-gpu')\n",
    "            # chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "            # chromeOptions.add_argument('--no-sandbox')\n",
    "            # chromeOptions.add_argument('--ignore-certificate-errors')\n",
    "            # chromeOptions.add_argument('no-sandbox')\n",
    "            chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "            browser = webdriver.Chrome(executable_path = path_to_chromedriver, options=chromeOptions)\n",
    "            print(\"scraping starts\")\n",
    "            browser.set_window_size(1800, 1000)\n",
    "            #Login\n",
    "            browser.get(url)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"login in time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_id('username').send_keys(username)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_id('password').send_keys(password)\n",
    "            browser.find_element_by_xpath('//*[@id=\"loginform\"]/div[4]/button').click()\n",
    "            print(\"{} login in successfully\".format(username))\n",
    "            # Get Page Info\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"redirect to search page time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"searchTerms\"]').send_keys(searchTerms)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_xpath('//*[@id=\"mainSearch\"]').click()\n",
    "            # Sort by Date and narrow by some conditions\n",
    "            #publication type\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting publication type conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonpublicationtype\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Newswires & Press Releases')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"publication type is set\")\n",
    "            #source\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting source conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsource\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                # browser.find_element_by_xpath('//*[@id=\"refine\"]/ul[1]/li[7]/button').click()\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business Wire')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_text_link('The Associated Press').click()\n",
    "                #browser.find_elements_by_xpath(\"//*[contains(text(), 'My Button')]\").click()\n",
    "                print(\"source is set\")\n",
    "            except:\n",
    "                pass\n",
    "            # timeline start\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting timeline time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"refine\"]/button[2]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/input').get_attribute('value').split('/')[-1]) <= 2001:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[2]/ol/li[33]/button').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2005')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'January')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[2]/td[3]/button').click()\n",
    "            # timeline end\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/input').get_attribute('value').split('/')[-1]) >= 2019:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2014')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'December')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[1]/ol/li[12]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[6]/td[2]/button').click()\n",
    "            # click ok\n",
    "            browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/button').click()\n",
    "            start_time = time.time()\n",
    "            print(\"time range is set\")\n",
    "            # subject\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting subject conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsubject\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business News')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"subject is set\")\n",
    "\n",
    "            #sort by time\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"sorting by time time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/button').click()\n",
    "                    break\n",
    "                except WebDriverException:\n",
    "                    time.sleep(1)\n",
    "            browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/div/button[4]').click()\n",
    "            #N_temp = how many articles eg: News (10,000+)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"get number of articles time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    N_temp = browser.find_element_by_xpath('//*[@id=\"content\"]/header/h2/span').text\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            total_number = int(''.join(re.findall(r'[0-9]', N_temp)))\n",
    "            print(\"we'll scrape down {} files related to '{}'\".format(total_number, searchTerms))\n",
    "            total_page = int(np.ceil(total_number/10))\n",
    "            file_digit = len(str(total_page)) * 2 + 1\n",
    "            for page in range(1, total_page + 1):\n",
    "                time.sleep(2)\n",
    "                date = browser.find_element_by_xpath('//*[@id=\"content\"]/div[2]/form/div[2]/ol/li[1]/div/div[1]/dl/dd[4]/a').text\n",
    "                month, year = date.split(',')\n",
    "                month = month_d[month[:3]]\n",
    "                new_folder = download_folder + r'\\{}\\{}{}'.format(searchTerms, month.strip(), year.strip())\n",
    "                #new_folder = download_folder + r'\\{}-{}\\{}'.format( month.strip(), year.strip(),searchTerms)\n",
    "                # =============================================================================\n",
    "                #     If the file already exists. Go to next page.\n",
    "                # =============================================================================\n",
    "                if os.path.isfile(new_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "                #if os.path.isfile(download_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "\n",
    "                    print('exist: ' + str(page) + '_' + str(total_page) +  '.ZIP')\n",
    "                    if page < total_page:\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            while True:\n",
    "                                if time.time() - start_time > dead_time:\n",
    "                                    raise Exception()\n",
    "                                WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.LINK_TEXT , str(page + 1))))\n",
    "                                browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        except WebDriverException:\n",
    "                            time.sleep(3)\n",
    "                            continue\n",
    "                # =============================================================================\n",
    "                # Wait the all checkbox to be clickable\n",
    "                # =============================================================================\n",
    "                while True:\n",
    "                    print(\"here\")\n",
    "                    try:\n",
    "                        ele = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input')\n",
    "                        print(ele)\n",
    "                        # if browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').get_attribute('checked') != 'true':\n",
    "                        #     browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').click()\n",
    "                        # print(ele.get_attribute('checked'))\n",
    "                        # if not ele.get_attribute('checked'):\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(1)\n",
    "                # =============================================================================\n",
    "                #   Wait the IncludeAttachments button to be clickable.  otherwise re-click download buttom\n",
    "                # =============================================================================\n",
    "                # try:\n",
    "                #     start_time = time.time()\n",
    "                #     while True:\n",
    "                #         if time.time() - start_time > dead_time:\n",
    "                #             raise Exception()\n",
    "                #         elm = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button')\n",
    "                #         elm.click()\n",
    "                # except WebDriverException:\n",
    "                #     pass\n",
    "                while True:\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"DocumentsOnly\"]').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.find_element_by_xpath('//*[@id=\"IncludeAttachments\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"Docx\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"SeparateFiles\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').clear()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').send_keys((str(page) + '_' + str(total_page)).zfill(file_digit))\n",
    "                # =============================================================================\n",
    "                #     After downloading Close the pop up window. LexisNexis only allows 5 cocurrent windows\n",
    "                # =============================================================================\n",
    "                before = browser.window_handles[0]\n",
    "                # browser.find_element_by_xpath('/html/body/aside/footer/ul/li[1]/input').click()\n",
    "                browser.find_element_by_xpath('/html/body/aside/footer/div/button[1]').click()\n",
    "                # print('here')\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        after = browser.window_handles[1]\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.switch_to.window(after)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    if  browser.find_elements_by_link_text((str(page) + '_' + str(total_page)).zfill(file_digit)):\n",
    "                        break\n",
    "                browser.close()\n",
    "                browser.switch_to.window(before)\n",
    "                print(browser.title)\n",
    "                time.sleep(2)\n",
    "                old_folder = download_folder + r'\\temp'\n",
    "                os.chdir(old_folder)\n",
    "                if not os.path.isdir(new_folder):\n",
    "                    os.makedirs(new_folder)\n",
    "                for f in os.listdir():\n",
    "                    shutil.move(old_folder + r'\\{}'.format(f), new_folder + r'\\{}'.format(f))\n",
    "                print('finishing page' + str(page) + '_' + str(total_page)  )\n",
    "                # =============================================================================\n",
    "                #     Go to the next page. Have to check the next page link is clickable\n",
    "                # =============================================================================\n",
    "                if page < total_page:\n",
    "                    try:\n",
    "                        browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        start_time = time.time()\n",
    "                        while True:\n",
    "                            if time.time() - start_time > dead_time:\n",
    "                                raise Exception()\n",
    "                            browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(3)\n",
    "                        pass\n",
    "                else:\n",
    "                    print('FINISHED!')\n",
    "                    sys.exit()\n",
    "                # except Exception:\n",
    "                #     print('restarting')\n",
    "                #     continue\n",
    "            print('Finished')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"restarting\")\n",
    "\n",
    "\n",
    "def unzip(download_folder):\n",
    "    if not os.path.exists(download_folder + '/' + 'unzipped'):\n",
    "        os.makedirs(download_folder + '/' + 'unzipped')\n",
    "    for filename in os.listdir(download_folder):\n",
    "        if not filename.endswith('.ZIP'):\n",
    "            continue\n",
    "        zip_ref = zipfile.ZipFile(download_folder +  '/' + filename, 'r')\n",
    "        zip_ref.extractall(download_folder + '/' + 'unzipped')\n",
    "        if len(zip_ref.namelist()) != 11:\n",
    "            print('missing document at ' + filename)\n",
    "        zip_ref.close()\n",
    "        print('unzipping ' + filename)\n",
    "\n",
    "\n",
    "def create_index(download_folder, searchTerms):\n",
    "    def get_docx_text(path):\n",
    "        WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "        PARA = WORD_NAMESPACE + 'p'\n",
    "        TEXT = WORD_NAMESPACE + 't'\n",
    "        \"\"\"\n",
    "        Take the path of a docx file as argument, return the text in unicode.\n",
    "        \"\"\"\n",
    "        document = zipfile.ZipFile(path)\n",
    "        xml_content = document.read('word/document.xml')\n",
    "        document.close()\n",
    "        tree = XML(xml_content)\n",
    "\n",
    "        paragraphs = []\n",
    "        for paragraph in tree.getiterator(PARA):\n",
    "            texts = [node.text\n",
    "                     for node in paragraph.getiterator(TEXT)\n",
    "                     if node.text]\n",
    "            if texts:\n",
    "                paragraphs.append(''.join(texts))\n",
    "\n",
    "        return '\\n\\n'.join(paragraphs)\n",
    "    def get_docx_hyperlink(path):\n",
    "        document = Document(path)\n",
    "        rels = document.part.rels\n",
    "        link = []\n",
    "        for rel in rels:\n",
    "            if rels[rel].reltype == RT.HYPERLINK:\n",
    "                link.append(rels[rel]._target)\n",
    "        return pd.Series(link)\n",
    "    def find_between(s, first, last):\n",
    "        list = []\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    start = s.index( first ) + len( first )\n",
    "                    end = s.index( last, start )\n",
    "                    list.append( s[start:end])\n",
    "                    s = s.replace(first + s[start:end] + last,'')\n",
    "                except:\n",
    "                    break\n",
    "            return list\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "\n",
    "    index = pd.DataFrame(({'Title':{},'Link':{}}))\n",
    "    for filename in os.listdir(download_folder + '/' + 'unzipped'):\n",
    "        if filename.find(\"doclist\") == -1:\n",
    "            continue\n",
    "        text = get_docx_text(download_folder + '/' + 'unzipped/' + filename)\n",
    "        link = get_docx_hyperlink(download_folder + '/' + 'unzipped/' + filename).loc[0]\n",
    "        content = '<start>' + re.sub(r'.*Documents \\(\\d*\\)', '',text.replace('\\n',' ')).replace(\\\n",
    "              'Client/Matter: -None-  Search Terms: '+ searchTerms + '  Search Type: Terms and Connectors   Narrowed by:   Content Type  Narrowed by  News  -None-',\\\n",
    "              '<end><start>')\n",
    "        content_list = pd.Series(find_between(content, '<start>', '<end>'))\n",
    "        content_list = content_list.str[5:]\n",
    "        index = index.append(pd.DataFrame({'Title':content_list,'Link':link}),ignore_index = True)\n",
    "    index = index.reset_index()\n",
    "    index['index'] = index.index + 1\n",
    "    index.to_csv(download_folder + '/index.csv', index=False)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for searchTerms in searchTermsList:\n",
    "        download_folder = root + r'\\download'\n",
    "        download_file(searchTerms = searchTerms, download_folder = download_folder)\n",
    "    # unzip()\n",
    "    # create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 12 17:31:39 2018\n",
    "\n",
    "@author: chongshu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from xml.etree.cElementTree import XML\n",
    "import sys\n",
    "\n",
    "month_d = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "#searchTerms = r'JP Morgan'\n",
    "searchTermsList = [r'oracle', r'morgan stanley']\n",
    "searchTermsList = map(lambda x: f'ticker({x})' ,tickers[1:2])\n",
    "url = r'https://libproxy.usc.edu/login?url=http://www.nexisuni.com'\n",
    "# url = r'http://libguides.usc.edu/go.php?c=9232127'\n",
    "username = 'junhuihe'\n",
    "password = 'ivy930322jasper951128'\n",
    "# username = 'MyUSCPassUsername'\n",
    "# password = 'MyUSCPassWord'\n",
    "#root = r'/Users/jasper/Documents/scrape_help/LexisNexis-Scraping'\n",
    "root = r'C:\\Users\\econguest\\Desktop\\scraper'\n",
    "path_to_chromedriver = root + r'\\chromedriver.exe'\n",
    "#download_folder = root + r'\\{}\\download'.format(searchTerms)\n",
    "# path_to_chromedriver = root + r'\\chromedriver'\n",
    "# download_folder = root + r'\\download'\n",
    "# dead_time = 300\n",
    "dead_time = 300\n",
    "\n",
    "\n",
    "def download_file(searchTerms, download_folder, url = url, username = username, \\\n",
    "                  dead_time = dead_time, path_to_chromedriver=path_to_chromedriver):\n",
    "    while True:\n",
    "        try:\n",
    "            # chromeOptions = webdriver.ChromeOptions()\n",
    "            prefs = {\"download.default_directory\" : download_folder + r'\\temp'}\n",
    "            # chromeOptions.add_argument('headless')\n",
    "            chromeOptions = webdriver.ChromeOptions()\n",
    "            # chromeOptions.add_argument(\"--window-size=1800,1000\")\n",
    "            # chromeOptions.add_argument(\"--disable-extensions\")\n",
    "            # chromeOptions.add_argument(\"--proxy-server='direct://'\")\n",
    "            # chromeOptions.add_argument(\"--proxy-bypass-list=*\")\n",
    "            # chromeOptions.add_argument(\"--start-maximized\")\n",
    "            # chromeOptions.add_argument('--headless')\n",
    "            # chromeOptions.add_argument('--disable-gpu')\n",
    "            # chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "            # chromeOptions.add_argument('--no-sandbox')\n",
    "            # chromeOptions.add_argument('--ignore-certificate-errors')\n",
    "            # chromeOptions.add_argument('no-sandbox')\n",
    "            chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "            browser = webdriver.Chrome(executable_path = path_to_chromedriver, options=chromeOptions)\n",
    "            print(\"scraping starts\")\n",
    "            browser.set_window_size(1800, 1000)\n",
    "            #Login\n",
    "            browser.get(url)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"login in time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_id('username').send_keys(username)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_id('password').send_keys(password)\n",
    "            browser.find_element_by_xpath('//*[@id=\"loginform\"]/div[4]/button').click()\n",
    "            print(\"{} login in successfully\".format(username))\n",
    "            # Get Page Info\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"redirect to search page time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"searchTerms\"]').send_keys(searchTerms)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_xpath('//*[@id=\"mainSearch\"]').click()\n",
    "            # Sort by Date and narrow by some conditions\n",
    "            #publication type\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting publication type conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonpublicationtype\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Newswires & Press Releases')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"publication type is set\")\n",
    "            #source\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting source conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsource\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                # browser.find_element_by_xpath('//*[@id=\"refine\"]/ul[1]/li[7]/button').click()\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business Wire')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_text_link('The Associated Press').click()\n",
    "                #browser.find_elements_by_xpath(\"//*[contains(text(), 'My Button')]\").click()\n",
    "                print(\"source is set\")\n",
    "            except:\n",
    "                pass\n",
    "            # timeline start\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting timeline time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"refine\"]/button[2]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/input').get_attribute('value').split('/')[-1]) <= 2001:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[2]/ol/li[33]/button').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2005')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'January')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[2]/td[3]/button').click()\n",
    "            # timeline end\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/input').get_attribute('value').split('/')[-1]) >= 2019:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2014')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'December')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[1]/ol/li[12]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[6]/td[2]/button').click()\n",
    "            # click ok\n",
    "            browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/button').click()\n",
    "            start_time = time.time()\n",
    "            print(\"time range is set\")\n",
    "            # subject\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting subject conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsubject\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business News')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"subject is set\")\n",
    "\n",
    "            #sort by time\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"sorting by time time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/button').click()\n",
    "                    break\n",
    "                except WebDriverException:\n",
    "                    time.sleep(1)\n",
    "            browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/div/button[4]').click()\n",
    "            #N_temp = how many articles eg: News (10,000+)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"get number of articles time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    N_temp = browser.find_element_by_xpath('//*[@id=\"content\"]/header/h2/span').text\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            total_number = int(''.join(re.findall(r'[0-9]', N_temp)))\n",
    "            print(\"we'll scrape down {} files related to '{}'\".format(total_number, searchTerms))\n",
    "            total_page = int(np.ceil(total_number/10))\n",
    "            file_digit = len(str(total_page)) * 2 + 1\n",
    "            for page in range(1, total_page + 1):\n",
    "                time.sleep(2)\n",
    "                date = browser.find_element_by_xpath('//*[@id=\"content\"]/div[2]/form/div[2]/ol/li[1]/div/div[1]/dl/dd[4]/a').text\n",
    "                month, year = date.split(',')\n",
    "                month = month_d[month[:3]]\n",
    "                new_folder = download_folder + r'\\{}\\{}{}'.format(searchTerms, month.strip(), year.strip())\n",
    "                #new_folder = download_folder + r'\\{}-{}\\{}'.format( month.strip(), year.strip(),searchTerms)\n",
    "                # =============================================================================\n",
    "                #     If the file already exists. Go to next page.\n",
    "                # =============================================================================\n",
    "                if os.path.isfile(new_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "                #if os.path.isfile(download_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "\n",
    "                    print('exist: ' + str(page) + '_' + str(total_page) +  '.ZIP')\n",
    "                    if page < total_page:\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            while True:\n",
    "                                if time.time() - start_time > dead_time:\n",
    "                                    raise Exception()\n",
    "                                WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.LINK_TEXT , str(page + 1))))\n",
    "                                browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        except WebDriverException:\n",
    "                            time.sleep(3)\n",
    "                            continue\n",
    "                # =============================================================================\n",
    "                # Wait the all checkbox to be clickable\n",
    "                # =============================================================================\n",
    "                while True:\n",
    "                    print(\"here\")\n",
    "                    try:\n",
    "                        ele = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input')\n",
    "                        print(ele)\n",
    "                        # if browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').get_attribute('checked') != 'true':\n",
    "                        #     browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').click()\n",
    "                        # print(ele.get_attribute('checked'))\n",
    "                        # if not ele.get_attribute('checked'):\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(1)\n",
    "                # =============================================================================\n",
    "                #   Wait the IncludeAttachments button to be clickable.  otherwise re-click download buttom\n",
    "                # =============================================================================\n",
    "                # try:\n",
    "                #     start_time = time.time()\n",
    "                #     while True:\n",
    "                #         if time.time() - start_time > dead_time:\n",
    "                #             raise Exception()\n",
    "                #         elm = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button')\n",
    "                #         elm.click()\n",
    "                # except WebDriverException:\n",
    "                #     pass\n",
    "                while True:\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"DocumentsOnly\"]').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.find_element_by_xpath('//*[@id=\"IncludeAttachments\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"Docx\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"SeparateFiles\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').clear()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').send_keys((str(page) + '_' + str(total_page)).zfill(file_digit))\n",
    "                # =============================================================================\n",
    "                #     After downloading Close the pop up window. LexisNexis only allows 5 cocurrent windows\n",
    "                # =============================================================================\n",
    "                before = browser.window_handles[0]\n",
    "                # browser.find_element_by_xpath('/html/body/aside/footer/ul/li[1]/input').click()\n",
    "                browser.find_element_by_xpath('/html/body/aside/footer/div/button[1]').click()\n",
    "                # print('here')\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        after = browser.window_handles[1]\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.switch_to.window(after)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    if  browser.find_elements_by_link_text((str(page) + '_' + str(total_page)).zfill(file_digit)):\n",
    "                        break\n",
    "                browser.close()\n",
    "                browser.switch_to.window(before)\n",
    "                print(browser.title)\n",
    "                time.sleep(2)\n",
    "                old_folder = download_folder + r'\\temp'\n",
    "                os.chdir(old_folder)\n",
    "                if not os.path.isdir(new_folder):\n",
    "                    os.makedirs(new_folder)\n",
    "                for f in os.listdir():\n",
    "                    shutil.move(old_folder + r'\\{}'.format(f), new_folder + r'\\{}'.format(f))\n",
    "                print('finishing page' + str(page) + '_' + str(total_page)  )\n",
    "                # =============================================================================\n",
    "                #     Go to the next page. Have to check the next page link is clickable\n",
    "                # =============================================================================\n",
    "                if page < total_page:\n",
    "                    try:\n",
    "                        browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        start_time = time.time()\n",
    "                        while True:\n",
    "                            if time.time() - start_time > dead_time:\n",
    "                                raise Exception()\n",
    "                            browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(3)\n",
    "                        pass\n",
    "                else:\n",
    "                    print('FINISHED!')\n",
    "                    sys.exit()\n",
    "                # except Exception:\n",
    "                #     print('restarting')\n",
    "                #     continue\n",
    "            print('Finished')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"restarting\")\n",
    "\n",
    "\n",
    "def unzip(download_folder):\n",
    "    if not os.path.exists(download_folder + '/' + 'unzipped'):\n",
    "        os.makedirs(download_folder + '/' + 'unzipped')\n",
    "    for filename in os.listdir(download_folder):\n",
    "        if not filename.endswith('.ZIP'):\n",
    "            continue\n",
    "        zip_ref = zipfile.ZipFile(download_folder +  '/' + filename, 'r')\n",
    "        zip_ref.extractall(download_folder + '/' + 'unzipped')\n",
    "        if len(zip_ref.namelist()) != 11:\n",
    "            print('missing document at ' + filename)\n",
    "        zip_ref.close()\n",
    "        print('unzipping ' + filename)\n",
    "\n",
    "\n",
    "def create_index(download_folder, searchTerms):\n",
    "    def get_docx_text(path):\n",
    "        WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "        PARA = WORD_NAMESPACE + 'p'\n",
    "        TEXT = WORD_NAMESPACE + 't'\n",
    "        \"\"\"\n",
    "        Take the path of a docx file as argument, return the text in unicode.\n",
    "        \"\"\"\n",
    "        document = zipfile.ZipFile(path)\n",
    "        xml_content = document.read('word/document.xml')\n",
    "        document.close()\n",
    "        tree = XML(xml_content)\n",
    "\n",
    "        paragraphs = []\n",
    "        for paragraph in tree.getiterator(PARA):\n",
    "            texts = [node.text\n",
    "                     for node in paragraph.getiterator(TEXT)\n",
    "                     if node.text]\n",
    "            if texts:\n",
    "                paragraphs.append(''.join(texts))\n",
    "\n",
    "        return '\\n\\n'.join(paragraphs)\n",
    "    def get_docx_hyperlink(path):\n",
    "        document = Document(path)\n",
    "        rels = document.part.rels\n",
    "        link = []\n",
    "        for rel in rels:\n",
    "            if rels[rel].reltype == RT.HYPERLINK:\n",
    "                link.append(rels[rel]._target)\n",
    "        return pd.Series(link)\n",
    "    def find_between(s, first, last):\n",
    "        list = []\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    start = s.index( first ) + len( first )\n",
    "                    end = s.index( last, start )\n",
    "                    list.append( s[start:end])\n",
    "                    s = s.replace(first + s[start:end] + last,'')\n",
    "                except:\n",
    "                    break\n",
    "            return list\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "\n",
    "    index = pd.DataFrame(({'Title':{},'Link':{}}))\n",
    "    for filename in os.listdir(download_folder + '/' + 'unzipped'):\n",
    "        if filename.find(\"doclist\") == -1:\n",
    "            continue\n",
    "        text = get_docx_text(download_folder + '/' + 'unzipped/' + filename)\n",
    "        link = get_docx_hyperlink(download_folder + '/' + 'unzipped/' + filename).loc[0]\n",
    "        content = '<start>' + re.sub(r'.*Documents \\(\\d*\\)', '',text.replace('\\n',' ')).replace(\\\n",
    "              'Client/Matter: -None-  Search Terms: '+ searchTerms + '  Search Type: Terms and Connectors   Narrowed by:   Content Type  Narrowed by  News  -None-',\\\n",
    "              '<end><start>')\n",
    "        content_list = pd.Series(find_between(content, '<start>', '<end>'))\n",
    "        content_list = content_list.str[5:]\n",
    "        index = index.append(pd.DataFrame({'Title':content_list,'Link':link}),ignore_index = True)\n",
    "    index = index.reset_index()\n",
    "    index['index'] = index.index + 1\n",
    "    index.to_csv(download_folder + '/index.csv', index=False)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for searchTerms in searchTermsList:\n",
    "        download_folder = root + r'\\download'\n",
    "        download_file(searchTerms = searchTerms, download_folder = download_folder)\n",
    "    # unzip()\n",
    "    # create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 12 17:31:39 2018\n",
    "\n",
    "@author: chongshu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from xml.etree.cElementTree import XML\n",
    "import sys\n",
    "\n",
    "month_d = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "#searchTerms = r'JP Morgan'\n",
    "searchTermsList = [r'oracle', r'morgan stanley']\n",
    "searchTermsList = map(lambda x: f'ticker({x})' ,tickers[2:3])\n",
    "url = r'https://libproxy.usc.edu/login?url=http://www.nexisuni.com'\n",
    "# url = r'http://libguides.usc.edu/go.php?c=9232127'\n",
    "username = 'junhuihe'\n",
    "password = 'ivy930322jasper951128'\n",
    "# username = 'MyUSCPassUsername'\n",
    "# password = 'MyUSCPassWord'\n",
    "#root = r'/Users/jasper/Documents/scrape_help/LexisNexis-Scraping'\n",
    "root = r'C:\\Users\\econguest\\Desktop\\scraper'\n",
    "path_to_chromedriver = root + r'\\chromedriver.exe'\n",
    "#download_folder = root + r'\\{}\\download'.format(searchTerms)\n",
    "# path_to_chromedriver = root + r'\\chromedriver'\n",
    "# download_folder = root + r'\\download'\n",
    "# dead_time = 300\n",
    "dead_time = 300\n",
    "\n",
    "\n",
    "def download_file(searchTerms, download_folder, url = url, username = username, \\\n",
    "                  dead_time = dead_time, path_to_chromedriver=path_to_chromedriver):\n",
    "    while True:\n",
    "        try:\n",
    "            # chromeOptions = webdriver.ChromeOptions()\n",
    "            prefs = {\"download.default_directory\" : download_folder + r'\\temp'}\n",
    "            # chromeOptions.add_argument('headless')\n",
    "            chromeOptions = webdriver.ChromeOptions()\n",
    "            # chromeOptions.add_argument(\"--window-size=1800,1000\")\n",
    "            # chromeOptions.add_argument(\"--disable-extensions\")\n",
    "            # chromeOptions.add_argument(\"--proxy-server='direct://'\")\n",
    "            # chromeOptions.add_argument(\"--proxy-bypass-list=*\")\n",
    "            # chromeOptions.add_argument(\"--start-maximized\")\n",
    "            # chromeOptions.add_argument('--headless')\n",
    "            # chromeOptions.add_argument('--disable-gpu')\n",
    "            # chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "            # chromeOptions.add_argument('--no-sandbox')\n",
    "            # chromeOptions.add_argument('--ignore-certificate-errors')\n",
    "            # chromeOptions.add_argument('no-sandbox')\n",
    "            chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "            browser = webdriver.Chrome(executable_path = path_to_chromedriver, options=chromeOptions)\n",
    "            print(\"scraping starts\")\n",
    "            browser.set_window_size(1800, 1000)\n",
    "            #Login\n",
    "            browser.get(url)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"login in time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_id('username').send_keys(username)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_id('password').send_keys(password)\n",
    "            browser.find_element_by_xpath('//*[@id=\"loginform\"]/div[4]/button').click()\n",
    "            print(\"{} login in successfully\".format(username))\n",
    "            # Get Page Info\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"redirect to search page time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"searchTerms\"]').send_keys(searchTerms)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_xpath('//*[@id=\"mainSearch\"]').click()\n",
    "            # Sort by Date and narrow by some conditions\n",
    "            #publication type\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting publication type conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonpublicationtype\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Newswires & Press Releases')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"publication type is set\")\n",
    "            #source\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting source conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsource\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                # browser.find_element_by_xpath('//*[@id=\"refine\"]/ul[1]/li[7]/button').click()\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business Wire')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_text_link('The Associated Press').click()\n",
    "                #browser.find_elements_by_xpath(\"//*[contains(text(), 'My Button')]\").click()\n",
    "                print(\"source is set\")\n",
    "            except:\n",
    "                pass\n",
    "            # timeline start\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting timeline time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"refine\"]/button[2]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/input').get_attribute('value').split('/')[-1]) <= 2001:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[2]/ol/li[33]/button').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2005')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'January')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[2]/td[3]/button').click()\n",
    "            # timeline end\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/input').get_attribute('value').split('/')[-1]) >= 2019:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2014')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'December')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[1]/ol/li[12]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[6]/td[2]/button').click()\n",
    "            # click ok\n",
    "            browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/button').click()\n",
    "            start_time = time.time()\n",
    "            print(\"time range is set\")\n",
    "            # subject\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting subject conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsubject\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business News')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"subject is set\")\n",
    "\n",
    "            #sort by time\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"sorting by time time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/button').click()\n",
    "                    break\n",
    "                except WebDriverException:\n",
    "                    time.sleep(1)\n",
    "            browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/div/button[4]').click()\n",
    "            #N_temp = how many articles eg: News (10,000+)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"get number of articles time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    N_temp = browser.find_element_by_xpath('//*[@id=\"content\"]/header/h2/span').text\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            total_number = int(''.join(re.findall(r'[0-9]', N_temp)))\n",
    "            print(\"we'll scrape down {} files related to '{}'\".format(total_number, searchTerms))\n",
    "            total_page = int(np.ceil(total_number/10))\n",
    "            file_digit = len(str(total_page)) * 2 + 1\n",
    "            for page in range(1, total_page + 1):\n",
    "                time.sleep(2)\n",
    "                date = browser.find_element_by_xpath('//*[@id=\"content\"]/div[2]/form/div[2]/ol/li[1]/div/div[1]/dl/dd[4]/a').text\n",
    "                month, year = date.split(',')\n",
    "                month = month_d[month[:3]]\n",
    "                new_folder = download_folder + r'\\{}\\{}{}'.format(searchTerms, month.strip(), year.strip())\n",
    "                #new_folder = download_folder + r'\\{}-{}\\{}'.format( month.strip(), year.strip(),searchTerms)\n",
    "                # =============================================================================\n",
    "                #     If the file already exists. Go to next page.\n",
    "                # =============================================================================\n",
    "                if os.path.isfile(new_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "                #if os.path.isfile(download_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "\n",
    "                    print('exist: ' + str(page) + '_' + str(total_page) +  '.ZIP')\n",
    "                    if page < total_page:\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            while True:\n",
    "                                if time.time() - start_time > dead_time:\n",
    "                                    raise Exception()\n",
    "                                WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.LINK_TEXT , str(page + 1))))\n",
    "                                browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        except WebDriverException:\n",
    "                            time.sleep(3)\n",
    "                            continue\n",
    "                # =============================================================================\n",
    "                # Wait the all checkbox to be clickable\n",
    "                # =============================================================================\n",
    "                while True:\n",
    "                    print(\"here\")\n",
    "                    try:\n",
    "                        ele = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input')\n",
    "                        print(ele)\n",
    "                        # if browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').get_attribute('checked') != 'true':\n",
    "                        #     browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').click()\n",
    "                        # print(ele.get_attribute('checked'))\n",
    "                        # if not ele.get_attribute('checked'):\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(1)\n",
    "                # =============================================================================\n",
    "                #   Wait the IncludeAttachments button to be clickable.  otherwise re-click download buttom\n",
    "                # =============================================================================\n",
    "                # try:\n",
    "                #     start_time = time.time()\n",
    "                #     while True:\n",
    "                #         if time.time() - start_time > dead_time:\n",
    "                #             raise Exception()\n",
    "                #         elm = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button')\n",
    "                #         elm.click()\n",
    "                # except WebDriverException:\n",
    "                #     pass\n",
    "                while True:\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"DocumentsOnly\"]').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.find_element_by_xpath('//*[@id=\"IncludeAttachments\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"Docx\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"SeparateFiles\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').clear()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').send_keys((str(page) + '_' + str(total_page)).zfill(file_digit))\n",
    "                # =============================================================================\n",
    "                #     After downloading Close the pop up window. LexisNexis only allows 5 cocurrent windows\n",
    "                # =============================================================================\n",
    "                before = browser.window_handles[0]\n",
    "                # browser.find_element_by_xpath('/html/body/aside/footer/ul/li[1]/input').click()\n",
    "                browser.find_element_by_xpath('/html/body/aside/footer/div/button[1]').click()\n",
    "                # print('here')\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        after = browser.window_handles[1]\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.switch_to.window(after)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    if  browser.find_elements_by_link_text((str(page) + '_' + str(total_page)).zfill(file_digit)):\n",
    "                        break\n",
    "                browser.close()\n",
    "                browser.switch_to.window(before)\n",
    "                print(browser.title)\n",
    "                time.sleep(2)\n",
    "                old_folder = download_folder + r'\\temp'\n",
    "                os.chdir(old_folder)\n",
    "                if not os.path.isdir(new_folder):\n",
    "                    os.makedirs(new_folder)\n",
    "                for f in os.listdir():\n",
    "                    shutil.move(old_folder + r'\\{}'.format(f), new_folder + r'\\{}'.format(f))\n",
    "                print('finishing page' + str(page) + '_' + str(total_page)  )\n",
    "                # =============================================================================\n",
    "                #     Go to the next page. Have to check the next page link is clickable\n",
    "                # =============================================================================\n",
    "                if page < total_page:\n",
    "                    try:\n",
    "                        browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        start_time = time.time()\n",
    "                        while True:\n",
    "                            if time.time() - start_time > dead_time:\n",
    "                                raise Exception()\n",
    "                            browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(3)\n",
    "                        pass\n",
    "                else:\n",
    "                    print('FINISHED!')\n",
    "                    sys.exit()\n",
    "                # except Exception:\n",
    "                #     print('restarting')\n",
    "                #     continue\n",
    "            print('Finished')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"restarting\")\n",
    "\n",
    "\n",
    "def unzip(download_folder):\n",
    "    if not os.path.exists(download_folder + '/' + 'unzipped'):\n",
    "        os.makedirs(download_folder + '/' + 'unzipped')\n",
    "    for filename in os.listdir(download_folder):\n",
    "        if not filename.endswith('.ZIP'):\n",
    "            continue\n",
    "        zip_ref = zipfile.ZipFile(download_folder +  '/' + filename, 'r')\n",
    "        zip_ref.extractall(download_folder + '/' + 'unzipped')\n",
    "        if len(zip_ref.namelist()) != 11:\n",
    "            print('missing document at ' + filename)\n",
    "        zip_ref.close()\n",
    "        print('unzipping ' + filename)\n",
    "\n",
    "\n",
    "def create_index(download_folder, searchTerms):\n",
    "    def get_docx_text(path):\n",
    "        WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "        PARA = WORD_NAMESPACE + 'p'\n",
    "        TEXT = WORD_NAMESPACE + 't'\n",
    "        \"\"\"\n",
    "        Take the path of a docx file as argument, return the text in unicode.\n",
    "        \"\"\"\n",
    "        document = zipfile.ZipFile(path)\n",
    "        xml_content = document.read('word/document.xml')\n",
    "        document.close()\n",
    "        tree = XML(xml_content)\n",
    "\n",
    "        paragraphs = []\n",
    "        for paragraph in tree.getiterator(PARA):\n",
    "            texts = [node.text\n",
    "                     for node in paragraph.getiterator(TEXT)\n",
    "                     if node.text]\n",
    "            if texts:\n",
    "                paragraphs.append(''.join(texts))\n",
    "\n",
    "        return '\\n\\n'.join(paragraphs)\n",
    "    def get_docx_hyperlink(path):\n",
    "        document = Document(path)\n",
    "        rels = document.part.rels\n",
    "        link = []\n",
    "        for rel in rels:\n",
    "            if rels[rel].reltype == RT.HYPERLINK:\n",
    "                link.append(rels[rel]._target)\n",
    "        return pd.Series(link)\n",
    "    def find_between(s, first, last):\n",
    "        list = []\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    start = s.index( first ) + len( first )\n",
    "                    end = s.index( last, start )\n",
    "                    list.append( s[start:end])\n",
    "                    s = s.replace(first + s[start:end] + last,'')\n",
    "                except:\n",
    "                    break\n",
    "            return list\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "\n",
    "    index = pd.DataFrame(({'Title':{},'Link':{}}))\n",
    "    for filename in os.listdir(download_folder + '/' + 'unzipped'):\n",
    "        if filename.find(\"doclist\") == -1:\n",
    "            continue\n",
    "        text = get_docx_text(download_folder + '/' + 'unzipped/' + filename)\n",
    "        link = get_docx_hyperlink(download_folder + '/' + 'unzipped/' + filename).loc[0]\n",
    "        content = '<start>' + re.sub(r'.*Documents \\(\\d*\\)', '',text.replace('\\n',' ')).replace(\\\n",
    "              'Client/Matter: -None-  Search Terms: '+ searchTerms + '  Search Type: Terms and Connectors   Narrowed by:   Content Type  Narrowed by  News  -None-',\\\n",
    "              '<end><start>')\n",
    "        content_list = pd.Series(find_between(content, '<start>', '<end>'))\n",
    "        content_list = content_list.str[5:]\n",
    "        index = index.append(pd.DataFrame({'Title':content_list,'Link':link}),ignore_index = True)\n",
    "    index = index.reset_index()\n",
    "    index['index'] = index.index + 1\n",
    "    index.to_csv(download_folder + '/index.csv', index=False)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for searchTerms in searchTermsList:\n",
    "        download_folder = root + r'\\download'\n",
    "        download_file(searchTerms = searchTerms, download_folder = download_folder)\n",
    "    # unzip()\n",
    "    # create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 12 17:31:39 2018\n",
    "\n",
    "@author: chongshu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from xml.etree.cElementTree import XML\n",
    "import sys\n",
    "\n",
    "month_d = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "#searchTerms = r'JP Morgan'\n",
    "searchTermsList = [r'oracle', r'morgan stanley']\n",
    "searchTermsList = map(lambda x: f'ticker({x})' ,tickers[:10])\n",
    "url = r'https://libproxy.usc.edu/login?url=http://www.nexisuni.com'\n",
    "# url = r'http://libguides.usc.edu/go.php?c=9232127'\n",
    "username = 'junhuihe'\n",
    "password = 'ivy930322jasper951128'\n",
    "# username = 'MyUSCPassUsername'\n",
    "# password = 'MyUSCPassWord'\n",
    "#root = r'/Users/jasper/Documents/scrape_help/LexisNexis-Scraping'\n",
    "root = r'C:\\Users\\econguest\\Desktop\\scraper'\n",
    "path_to_chromedriver = root + r'\\chromedriver.exe'\n",
    "#download_folder = root + r'\\{}\\download'.format(searchTerms)\n",
    "# path_to_chromedriver = root + r'\\chromedriver'\n",
    "# download_folder = root + r'\\download'\n",
    "# dead_time = 300\n",
    "dead_time = 300\n",
    "\n",
    "\n",
    "def download_file(searchTerms, download_folder, url = url, username = username, \\\n",
    "                  dead_time = dead_time, path_to_chromedriver=path_to_chromedriver):\n",
    "    while True:\n",
    "        try:\n",
    "            # chromeOptions = webdriver.ChromeOptions()\n",
    "            prefs = {\"download.default_directory\" : download_folder + r'\\temp'}\n",
    "            # chromeOptions.add_argument('headless')\n",
    "            chromeOptions = webdriver.ChromeOptions()\n",
    "            # chromeOptions.add_argument(\"--window-size=1800,1000\")\n",
    "            # chromeOptions.add_argument(\"--disable-extensions\")\n",
    "            # chromeOptions.add_argument(\"--proxy-server='direct://'\")\n",
    "            # chromeOptions.add_argument(\"--proxy-bypass-list=*\")\n",
    "            # chromeOptions.add_argument(\"--start-maximized\")\n",
    "            # chromeOptions.add_argument('--headless')\n",
    "            # chromeOptions.add_argument('--disable-gpu')\n",
    "            # chromeOptions.add_argument('--disable-dev-shm-usage')\n",
    "            # chromeOptions.add_argument('--no-sandbox')\n",
    "            # chromeOptions.add_argument('--ignore-certificate-errors')\n",
    "            # chromeOptions.add_argument('no-sandbox')\n",
    "            chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "            browser = webdriver.Chrome(executable_path = path_to_chromedriver, options=chromeOptions)\n",
    "            print(\"scraping starts\")\n",
    "            browser.set_window_size(1800, 1000)\n",
    "            #Login\n",
    "            browser.get(url)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"login in time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_id('username').send_keys(username)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_id('password').send_keys(password)\n",
    "            browser.find_element_by_xpath('//*[@id=\"loginform\"]/div[4]/button').click()\n",
    "            print(\"{} login in successfully\".format(username))\n",
    "            # Get Page Info\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"redirect to search page time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"searchTerms\"]').send_keys(searchTerms)\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            browser.find_element_by_xpath('//*[@id=\"mainSearch\"]').click()\n",
    "            # Sort by Date and narrow by some conditions\n",
    "            #publication type\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting publication type conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonpublicationtype\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Newswires & Press Releases')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"publication type is set\")\n",
    "            #source\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting source conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsource\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            try:\n",
    "                # browser.find_element_by_xpath('//*[@id=\"refine\"]/ul[1]/li[7]/button').click()\n",
    "                browser.find_element_by_class_name('more').click()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business Wire')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_text_link('The Associated Press').click()\n",
    "                #browser.find_elements_by_xpath(\"//*[contains(text(), 'My Button')]\").click()\n",
    "                print(\"source is set\")\n",
    "            except:\n",
    "                pass\n",
    "            # timeline start\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting timeline time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"refine\"]/button[2]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/input').get_attribute('value').split('/')[-1]) <= 2001:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[1]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[2]/ol/li[33]/button').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2005')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'January')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[2]/td[3]/button').click()\n",
    "            # timeline end\n",
    "            if int(browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/input').get_attribute('value').split('/')[-1]) >= 2019:\n",
    "                browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/div[2]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectyeartimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), '2014')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'December')]\"):\n",
    "                    try:\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                #browser.find_element_by_xpath('//*[@id=\"selectmonthtimeline\"]').click()\n",
    "                #browser.find_element_by_xpath('//*[@id=\"datepicker\"]/h3/div[1]/ol/li[12]/button').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"datepicker\"]/table/tbody/tr[6]/td[2]/button').click()\n",
    "            # click ok\n",
    "            browser.find_element_by_xpath('//*[@id=\"refine\"]/div[2]/div[4]/button').click()\n",
    "            start_time = time.time()\n",
    "            print(\"time range is set\")\n",
    "            # subject\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"setting subject conditions timeout\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"podfiltersbuttonsubject\"]').click()\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            while True:\n",
    "                try:\n",
    "                    for ele in browser.find_elements_by_xpath(\"//*[contains(text(), 'Business News')]\"):\n",
    "                        try:\n",
    "                            ele.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            pass\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            print(\"subject is set\")\n",
    "\n",
    "            #sort by time\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"sorting by time time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/button').click()\n",
    "                    break\n",
    "                except WebDriverException:\n",
    "                    time.sleep(1)\n",
    "            browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[2]/li/div/div/button[4]').click()\n",
    "            #N_temp = how many articles eg: News (10,000+)\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                if time.time() - start > dead_time:\n",
    "                    print(\"get number of articles time out\")\n",
    "                    raise Exception()\n",
    "                try:\n",
    "                    N_temp = browser.find_element_by_xpath('//*[@id=\"content\"]/header/h2/span').text\n",
    "                    break\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "            total_number = int(''.join(re.findall(r'[0-9]', N_temp)))\n",
    "            print(\"we'll scrape down {} files related to '{}'\".format(total_number, searchTerms))\n",
    "            total_page = int(np.ceil(total_number/10))\n",
    "            file_digit = len(str(total_page)) * 2 + 1\n",
    "            for page in range(1, total_page + 1):\n",
    "                time.sleep(2)\n",
    "                date = browser.find_element_by_xpath('//*[@id=\"content\"]/div[2]/form/div[2]/ol/li[1]/div/div[1]/dl/dd[4]/a').text\n",
    "                month, year = date.split(',')\n",
    "                month = month_d[month[:3]]\n",
    "                new_folder = download_folder + r'\\{}\\{}{}'.format(searchTerms, month.strip(), year.strip())\n",
    "                #new_folder = download_folder + r'\\{}-{}\\{}'.format( month.strip(), year.strip(),searchTerms)\n",
    "                # =============================================================================\n",
    "                #     If the file already exists. Go to next page.\n",
    "                # =============================================================================\n",
    "                if os.path.isfile(new_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "                #if os.path.isfile(download_folder + '/'  + (str(page) + '_' + str(total_page)).zfill(file_digit) +  '.ZIP'):\n",
    "\n",
    "                    print('exist: ' + str(page) + '_' + str(total_page) +  '.ZIP')\n",
    "                    if page < total_page:\n",
    "                        try:\n",
    "                            start_time = time.time()\n",
    "                            while True:\n",
    "                                if time.time() - start_time > dead_time:\n",
    "                                    raise Exception()\n",
    "                                WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.LINK_TEXT , str(page + 1))))\n",
    "                                browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        except WebDriverException:\n",
    "                            time.sleep(3)\n",
    "                            continue\n",
    "                # =============================================================================\n",
    "                # Wait the all checkbox to be clickable\n",
    "                # =============================================================================\n",
    "                while True:\n",
    "                    print(\"here\")\n",
    "                    try:\n",
    "                        ele = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input')\n",
    "                        print(ele)\n",
    "                        # if browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').get_attribute('checked') != 'true':\n",
    "                        #     browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[1]/input').click()\n",
    "                        # print(ele.get_attribute('checked'))\n",
    "                        # if not ele.get_attribute('checked'):\n",
    "                        ele.click()\n",
    "                        break\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(1)\n",
    "                # =============================================================================\n",
    "                #   Wait the IncludeAttachments button to be clickable.  otherwise re-click download buttom\n",
    "                # =============================================================================\n",
    "                # try:\n",
    "                #     start_time = time.time()\n",
    "                #     while True:\n",
    "                #         if time.time() - start_time > dead_time:\n",
    "                #             raise Exception()\n",
    "                #         elm = browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button')\n",
    "                #         elm.click()\n",
    "                # except WebDriverException:\n",
    "                #     pass\n",
    "                while True:\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"results-list-delivery-toolbar\"]/div/ul[1]/li[4]/ul/li[3]/button').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    try:\n",
    "                        browser.find_element_by_xpath('//*[@id=\"DocumentsOnly\"]').click()\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.find_element_by_xpath('//*[@id=\"IncludeAttachments\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"Docx\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"SeparateFiles\"]').click()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').clear()\n",
    "                browser.find_element_by_xpath('//*[@id=\"FileName\"]').send_keys((str(page) + '_' + str(total_page)).zfill(file_digit))\n",
    "                # =============================================================================\n",
    "                #     After downloading Close the pop up window. LexisNexis only allows 5 cocurrent windows\n",
    "                # =============================================================================\n",
    "                before = browser.window_handles[0]\n",
    "                # browser.find_element_by_xpath('/html/body/aside/footer/ul/li[1]/input').click()\n",
    "                browser.find_element_by_xpath('/html/body/aside/footer/div/button[1]').click()\n",
    "                # print('here')\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        after = browser.window_handles[1]\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                browser.switch_to.window(after)\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    if time.time() - start_time > dead_time:\n",
    "                        raise Exception()\n",
    "                    if  browser.find_elements_by_link_text((str(page) + '_' + str(total_page)).zfill(file_digit)):\n",
    "                        break\n",
    "                browser.close()\n",
    "                browser.switch_to.window(before)\n",
    "                print(browser.title)\n",
    "                time.sleep(2)\n",
    "                old_folder = download_folder + r'\\temp'\n",
    "                os.chdir(old_folder)\n",
    "                if not os.path.isdir(new_folder):\n",
    "                    os.makedirs(new_folder)\n",
    "                for f in os.listdir():\n",
    "                    shutil.move(old_folder + r'\\{}'.format(f), new_folder + r'\\{}'.format(f))\n",
    "                print('finishing page' + str(page) + '_' + str(total_page)  )\n",
    "                # =============================================================================\n",
    "                #     Go to the next page. Have to check the next page link is clickable\n",
    "                # =============================================================================\n",
    "                if page < total_page:\n",
    "                    try:\n",
    "                        browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                        start_time = time.time()\n",
    "                        while True:\n",
    "                            if time.time() - start_time > dead_time:\n",
    "                                raise Exception()\n",
    "                            browser.find_element_by_link_text(str(page + 1)).click()\n",
    "                    except WebDriverException:\n",
    "                        time.sleep(3)\n",
    "                        pass\n",
    "                else:\n",
    "                    print('FINISHED!')\n",
    "                    sys.exit()\n",
    "                # except Exception:\n",
    "                #     print('restarting')\n",
    "                #     continue\n",
    "            print('Finished')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"restarting\")\n",
    "\n",
    "\n",
    "def unzip(download_folder):\n",
    "    if not os.path.exists(download_folder + '/' + 'unzipped'):\n",
    "        os.makedirs(download_folder + '/' + 'unzipped')\n",
    "    for filename in os.listdir(download_folder):\n",
    "        if not filename.endswith('.ZIP'):\n",
    "            continue\n",
    "        zip_ref = zipfile.ZipFile(download_folder +  '/' + filename, 'r')\n",
    "        zip_ref.extractall(download_folder + '/' + 'unzipped')\n",
    "        if len(zip_ref.namelist()) != 11:\n",
    "            print('missing document at ' + filename)\n",
    "        zip_ref.close()\n",
    "        print('unzipping ' + filename)\n",
    "\n",
    "\n",
    "def create_index(download_folder, searchTerms):\n",
    "    def get_docx_text(path):\n",
    "        WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "        PARA = WORD_NAMESPACE + 'p'\n",
    "        TEXT = WORD_NAMESPACE + 't'\n",
    "        \"\"\"\n",
    "        Take the path of a docx file as argument, return the text in unicode.\n",
    "        \"\"\"\n",
    "        document = zipfile.ZipFile(path)\n",
    "        xml_content = document.read('word/document.xml')\n",
    "        document.close()\n",
    "        tree = XML(xml_content)\n",
    "\n",
    "        paragraphs = []\n",
    "        for paragraph in tree.getiterator(PARA):\n",
    "            texts = [node.text\n",
    "                     for node in paragraph.getiterator(TEXT)\n",
    "                     if node.text]\n",
    "            if texts:\n",
    "                paragraphs.append(''.join(texts))\n",
    "\n",
    "        return '\\n\\n'.join(paragraphs)\n",
    "    def get_docx_hyperlink(path):\n",
    "        document = Document(path)\n",
    "        rels = document.part.rels\n",
    "        link = []\n",
    "        for rel in rels:\n",
    "            if rels[rel].reltype == RT.HYPERLINK:\n",
    "                link.append(rels[rel]._target)\n",
    "        return pd.Series(link)\n",
    "    def find_between(s, first, last):\n",
    "        list = []\n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    start = s.index( first ) + len( first )\n",
    "                    end = s.index( last, start )\n",
    "                    list.append( s[start:end])\n",
    "                    s = s.replace(first + s[start:end] + last,'')\n",
    "                except:\n",
    "                    break\n",
    "            return list\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "\n",
    "    index = pd.DataFrame(({'Title':{},'Link':{}}))\n",
    "    for filename in os.listdir(download_folder + '/' + 'unzipped'):\n",
    "        if filename.find(\"doclist\") == -1:\n",
    "            continue\n",
    "        text = get_docx_text(download_folder + '/' + 'unzipped/' + filename)\n",
    "        link = get_docx_hyperlink(download_folder + '/' + 'unzipped/' + filename).loc[0]\n",
    "        content = '<start>' + re.sub(r'.*Documents \\(\\d*\\)', '',text.replace('\\n',' ')).replace(\\\n",
    "              'Client/Matter: -None-  Search Terms: '+ searchTerms + '  Search Type: Terms and Connectors   Narrowed by:   Content Type  Narrowed by  News  -None-',\\\n",
    "              '<end><start>')\n",
    "        content_list = pd.Series(find_between(content, '<start>', '<end>'))\n",
    "        content_list = content_list.str[5:]\n",
    "        index = index.append(pd.DataFrame({'Title':content_list,'Link':link}),ignore_index = True)\n",
    "    index = index.reset_index()\n",
    "    index['index'] = index.index + 1\n",
    "    index.to_csv(download_folder + '/index.csv', index=False)\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for searchTerms in searchTermsList:\n",
    "        download_folder = root + r'\\download'\n",
    "        download_file(searchTerms = searchTerms, download_folder = download_folder)\n",
    "    # unzip()\n",
    "    # create_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
